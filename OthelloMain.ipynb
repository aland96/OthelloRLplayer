{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import math\n",
    "%run AIPlayer.ipynb\n",
    "%run SmallGame.ipynb\n",
    "%run Game.ipynb\n",
    "%run DDQNAgent.ipynb\n",
    "%run SmallAgent.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training phase for 8x8 Agent\n",
    "def initialize_game(agent):\n",
    "    print(\"Starting random agent:\")\n",
    "    #creating memory for PER with random matches\n",
    "    i=0\n",
    "    while i < agent.memory_cap:\n",
    "        b=Board(verbose=False)\n",
    "        pl=AI(0)\n",
    "        opp=AI(0)\n",
    "        d=b.get_game_over()\n",
    "        while d==False and i< agent.memory_cap:\n",
    "            state_init1 = agent.get_state(b)\n",
    "            action=pl.move(b)\n",
    "            reward=agent.set_reward(b,action)\n",
    "            b.coord_move(action)\n",
    "            d=b.get_game_over()\n",
    "            if(d==False):\n",
    "                b.coord_move(opp.move(b))\n",
    "                next_state= agent.get_state(b)\n",
    "                d=b.get_game_over()\n",
    "            else:\n",
    "                next_state=np.zeros([64])\n",
    "            next_state= agent.get_state(b)\n",
    "            d=b.get_game_over()\n",
    "            sample=(state_init1, action,reward, next_state, d)\n",
    "            #I use the reward as error to remember in the memory\n",
    "            error=abs(sample[2])\n",
    "            i+=1\n",
    "            agent.memory.add(error,sample)\n",
    "    print(\"Ending of Random creating memory\")\n",
    "    \n",
    "    \n",
    "def plot_seaborn_score(array_counter, array_score):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_score])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='score')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_win(array_counter, array_win):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_win])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='wins')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_predicted(array_counter, array_pred):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_pred])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='number of predicted moves')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_wrong(array_counter, array_wrong):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_wrong])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='times where i did a not valid move')\n",
    "    plt.show()\n",
    "    \n",
    "def get_record(score, record):\n",
    "        if score >= record:\n",
    "            return score\n",
    "        else:\n",
    "            return record \n",
    "    \n",
    "def run():\n",
    "\n",
    "    agent = DQNAgent()\n",
    "    print(agent.model.summary())\n",
    "    counter_games = 0\n",
    "    counter_move = 0\n",
    "    counter_notfin = 0\n",
    "    score_plot = []\n",
    "    win_plot = []\n",
    "    wrong_plot = []\n",
    "    counter_plot =[]\n",
    "    counter_predicted=[]\n",
    "    record = 0\n",
    "    #filling the memory\n",
    "    initialize_game(agent)\n",
    "        \n",
    "    while counter_games < 8000:\n",
    "        # Initialize classes\n",
    "        board=Board(verbose=False)\n",
    "        opponent = AI(0)\n",
    "        counter_pred = 0        \n",
    "        wrong_move=False\n",
    "        \n",
    "        done=board.get_game_over()\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            #get old state\n",
    "            state_old = agent.get_state(board)\n",
    "            counter_move +=1\n",
    "            \n",
    "            #predict the move of the agent\n",
    "            if random.random() < agent.epsilon:\n",
    "                if(agent.epsilon < 0.01):\n",
    "                    print(\"Possible move:\")\n",
    "                    state_old = agent.get_state(board)\n",
    "                    possible_moves = board.get_valid_moves()  \n",
    "                    possible_moves=list(zip(*possible_moves))\n",
    "                    weights = np.zeros(len(possible_moves[0]))\n",
    "                    p = np.exp(weights)\n",
    "                    p /= np.sum(p)\n",
    "                    choice = np.random.choice(range(len(weights)), p=p)\n",
    "                    action = possible_moves[0][choice]\n",
    "                else:\n",
    "                    print(\"Random move:\")\n",
    "                    #giving out also wrong possibilities to get also their values for the q-function\n",
    "                    mov=random.randint(0,63)\n",
    "                    row=mov//8\n",
    "                    column=mov%8\n",
    "                    action=(row,column)\n",
    "            else:\n",
    "                print(\"Predicted move:\")\n",
    "                counter_pred +=1\n",
    "                state_old = agent.get_state(board)\n",
    "                pred=agent.predict(state_old.reshape((1,64)))[0]\n",
    "                mov=np.argmax(pred)\n",
    "                row=mov//8            \n",
    "                column=mov%8\n",
    "                action=(row,column)\n",
    "    \n",
    "            #get the reward of the move predicted\n",
    "            reward= agent.set_reward(board,action)\n",
    "                \n",
    "            #perform new move and get new state\n",
    "            #I also get if the game is ended or not\n",
    "            print(action)\n",
    "            try:\n",
    "                board.coord_move(action)\n",
    "                done=board.get_game_over()\n",
    "            except:\n",
    "                print(\"Player did a not valid move\")\n",
    "                #ending manually the loop\n",
    "                wrong_move=True\n",
    "                done=True\n",
    "                \n",
    "            #opponent make move\n",
    "            #the next state is the state after the move of the opponent so it's again the move of the agent\n",
    "            if(done==False):\n",
    "                board.coord_move(opponent.move(board))\n",
    "                next_state= agent.get_state(board)\n",
    "                #inspecting if the game ended after the opponent move\n",
    "                done=board.get_game_over()\n",
    "            else:\n",
    "                next_state=np.zeros([64])\n",
    "            \n",
    "            # store the new data into a long term memory for the all game\n",
    "            agent.remember(state_old, action,reward, next_state, done)\n",
    "            \n",
    "            #train with replay new\n",
    "            agent.replay()\n",
    "            \n",
    "            #save score for final plot\n",
    "            record = get_record(board.get_black_score(), record)           \n",
    "            \n",
    "        counter_games += 1\n",
    "        print('Game', counter_games, '      Score:', board.get_black_score())\n",
    "        score_plot.append(board.get_black_score())\n",
    "        if board.get_black_score()>=board.get_white_score():\n",
    "            if wrong_move==True:\n",
    "                #I did a non valid move and just for a case I had more points but i lost\n",
    "                win_plot.append(0)\n",
    "                wrong_plot.append(1)\n",
    "            else:\n",
    "                win_plot.append(1)\n",
    "                wrong_plot.append(0)\n",
    "        else:\n",
    "            win_plot.append(0)\n",
    "            wrong_plot.append(0)\n",
    "        counter_plot.append(counter_games)\n",
    "        counter_predicted.append(counter_pred)\n",
    "    agent.model.save_weights('8x8_weights_new.hdf5')\n",
    "    plot_seaborn_score(counter_plot, score_plot)\n",
    "    plot_seaborn_win(counter_plot,win_plot)\n",
    "    plot_seaborn_predicted(counter_plot,counter_predicted)\n",
    "    plot_seaborn_wrong(counter_plot,wrong_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training phase for 4x4 Agent\n",
    "def initialize_gameS(agent):\n",
    "    print(\"Starting random agent:\")\n",
    "    #creating memory for PER with random matches\n",
    "    i=0\n",
    "    while i < agent.memory_cap:\n",
    "        b=SmallBoard(verbose=False)\n",
    "        #b=Board(verbose=False)\n",
    "        pl=AI(0)\n",
    "        opp=AI(0)\n",
    "        d=b.get_game_over()\n",
    "        while d==False and i< agent.memory_cap:\n",
    "            state_init1 = agent.get_state(b)\n",
    "            action=pl.move(b)\n",
    "            reward=agent.set_reward(b,action)\n",
    "            b.coord_move(action)\n",
    "            d=b.get_game_over()\n",
    "            if(d==False):\n",
    "                b.coord_move(opp.move(b))\n",
    "                next_state= agent.get_state(b)\n",
    "                d=b.get_game_over()\n",
    "            else:\n",
    "                next_state=np.zeros([16])\n",
    "            next_state= agent.get_state(b)\n",
    "            d=b.get_game_over()\n",
    "            sample=(state_init1, action,reward, next_state, d)\n",
    "            #I use the reward as error to remember in the memory\n",
    "            error=abs(sample[2])\n",
    "            i+=1\n",
    "            agent.memory.add(error,sample)\n",
    "    print(\"Ending of Random creating memory\")\n",
    "    \n",
    "    \n",
    "def plot_seaborn_scoreS(array_counter, array_score):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_score])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='score')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_winS(array_counter, array_win):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_win])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='wins')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_predictedS(array_counter, array_pred):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_pred])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='number of predicted moves')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_wrongS(array_counter, array_wrong):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_wrong])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='times where i did a not valid move')\n",
    "    plt.show()\n",
    "    \n",
    "def get_recordS(score, record):\n",
    "        if score >= record:\n",
    "            return score\n",
    "        else:\n",
    "            return record \n",
    "    \n",
    "def runS():\n",
    "\n",
    "    agent = DQNSmallAgent()\n",
    "    print(agent.model.summary())\n",
    "    counter_games = 0\n",
    "    counter_move = 0\n",
    "    counter_notfin = 0\n",
    "    score_plot = []\n",
    "    win_plot = []\n",
    "    wrong_plot = []\n",
    "    counter_plot =[]\n",
    "    counter_predicted=[]\n",
    "    record = 0\n",
    "    #filling the memory\n",
    "    initialize_gameS(agent)\n",
    "        \n",
    "    while counter_games < 7000:\n",
    "        # Initialize classes\n",
    "        board = SmallBoard(verbose=False)\n",
    "        #b=Board(verbose=False)\n",
    "        opponent = AI(0)\n",
    "        counter_pred = 0        \n",
    "        wrong_move=False\n",
    "        \n",
    "        done=board.get_game_over()\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            #get old state\n",
    "            state_old = agent.get_state(board)\n",
    "            counter_move +=1\n",
    "            \n",
    "            #predict the move of the agent\n",
    "            #action= agent.makemove(board)\n",
    "            if random.random() < agent.epsilon:\n",
    "                if(agent.epsilon < 0.01):\n",
    "                    print(\"Possible move:\")\n",
    "                    state_old = agent.get_state(board)\n",
    "                    possible_moves = board.get_valid_moves()  \n",
    "                    possible_moves=list(zip(*possible_moves))\n",
    "                    weights = np.zeros(len(possible_moves[0]))\n",
    "                    p = np.exp(weights)\n",
    "                    p /= np.sum(p)\n",
    "                    choice = np.random.choice(range(len(weights)), p=p)\n",
    "                    action = possible_moves[0][choice]\n",
    "                else:\n",
    "                    print(\"Random move:\")\n",
    "                    #giving out also wrong possibilities to get also their values for the q-function\n",
    "                    mov=random.randint(0,15)\n",
    "                    row=mov//4\n",
    "                    column=mov%4\n",
    "                    action=(row,column)\n",
    "            else:\n",
    "                print(\"Predicted move:\")\n",
    "                counter_pred +=1\n",
    "                state_old = agent.get_state(board)\n",
    "                pred=agent.predict(state_old.reshape((1,16)))[0]\n",
    "                mov=np.argmax(pred)\n",
    "                row=mov//4\n",
    "                column=mov%4\n",
    "                action=(row,column)\n",
    "    \n",
    "            #get the reward of the move predicted\n",
    "            reward= agent.set_reward(board,action)\n",
    "                \n",
    "            #perform new move and get new state\n",
    "            #I also get if the game is ended or not\n",
    "            print(action)\n",
    "            try:\n",
    "                board.coord_move(action)\n",
    "                done=board.get_game_over()\n",
    "            except:\n",
    "                print(\"Player did a not valid move\")\n",
    "                #ending manually the loop\n",
    "                wrong_move=True\n",
    "                done=True\n",
    "                \n",
    "            #opponent make move\n",
    "            #the next state is the state after the move of the opponent so it's again the move of the agent\n",
    "            if(done==False):\n",
    "                board.coord_move(opponent.move(board))\n",
    "                next_state= agent.get_state(board)\n",
    "                #inspecting if the game ended after the opponent move\n",
    "                done=board.get_game_over()\n",
    "            else:\n",
    "                next_state=np.zeros([16])\n",
    "            \n",
    "            # store the new data into a long term memory for the all game\n",
    "            agent.remember(state_old, action,reward, next_state, done)\n",
    "            \n",
    "            #train with replay new\n",
    "            agent.replay()\n",
    "            \n",
    "            #save score for final plot\n",
    "            record = get_recordS(board.get_black_score(), record)           \n",
    "            \n",
    "        counter_games += 1\n",
    "        print('Game', counter_games, '      Score:', board.get_black_score())\n",
    "        score_plot.append(board.get_black_score())\n",
    "        if board.get_black_score()>=board.get_white_score():\n",
    "            if wrong_move==True:\n",
    "                #I did a non valid move and just for a case I had more points but i lost\n",
    "                win_plot.append(0)\n",
    "                wrong_plot.append(1)\n",
    "            else:\n",
    "                win_plot.append(1)\n",
    "                wrong_plot.append(0)\n",
    "        else:\n",
    "            win_plot.append(0)\n",
    "            wrong_plot.append(0)\n",
    "        counter_plot.append(counter_games)\n",
    "        counter_predicted.append(counter_pred)\n",
    "    agent.model.save_weights('4x4_weights_new.hdf5')\n",
    "    plot_seaborn_scoreS(counter_plot, score_plot)\n",
    "    plot_seaborn_winS(counter_plot,win_plot)\n",
    "    plot_seaborn_predictedS(counter_plot,counter_predicted)\n",
    "    plot_seaborn_wrongS(counter_plot,wrong_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Games against random\n",
    "def runGames():\n",
    "    b=SmallBoard()\n",
    "    agent = DQNSmallAgent(\"4x4_weights_new.hdf5\")\n",
    "    opp=AI(0)\n",
    "    d=b.get_game_over()\n",
    "    i=0\n",
    "    counter_games=0\n",
    "    win_plot=[]\n",
    "    counter_plot=[]\n",
    "    counter_wrong=0\n",
    "    counter_pred=0\n",
    "    wrong_move=False\n",
    "    while counter_games < 1000:\n",
    "        wrong_move=False\n",
    "        i=0\n",
    "        b=SmallBoard()\n",
    "        d=b.get_game_over()\n",
    "        while d==False:\n",
    "            if(i==0):\n",
    "                try:\n",
    "                    move=agent.makemove(b)\n",
    "                    print(move)\n",
    "                    b.coord_move(move)\n",
    "                    counter_pred+=1\n",
    "                except:\n",
    "                    print(\"Player did a not valid move\")\n",
    "                    #ending manually the loop\n",
    "                    wrong_move=True\n",
    "                    counter_wrong+=1\n",
    "                    counter_pred-=1\n",
    "                    d=True\n",
    "                i=1\n",
    "            else:\n",
    "                b.coord_move(opp.move(b))\n",
    "                i=0\n",
    "            if(d==False):\n",
    "                d=b.get_game_over()\n",
    "        if b.get_black_score() >= b.get_white_score():\n",
    "            if(wrong_move==False):\n",
    "                win_plot.append(1) \n",
    "            else:\n",
    "                win_plot.append(0) \n",
    "        else:\n",
    "                win_plot.append(0)\n",
    "        counter_games+=1\n",
    "        counter_plot.append(counter_games)\n",
    "    plot_seaborn_win(counter_plot,win_plot)\n",
    "    print(\"The percentage of wrong moves is {:.4f}\".format(counter_wrong/counter_pred))\n",
    "    print(\"The number of wrong moves is {}\".format(counter_wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Q-function\n",
    "def plotQ():\n",
    "    b=SmallBoard()\n",
    "    agent = DQNSmallAgent(\"4x4_randomweights_3layers_good.hdf5\")\n",
    "    agentBad =DQNSmallAgent(\"4x4_weights_smalltrain.hdf5\")\n",
    "    b.human_move(\"B1\")\n",
    "    b.human_move(\"A3\")\n",
    "    state_old = agent.get_state(b)\n",
    "    pred=agent.model.predict(state_old.reshape((1,16)))[0]\n",
    "    predBad=agentBad.model.predict(state_old.reshape((1,16)))[0]\n",
    "    mov=np.argmax(pred)\n",
    "    row=mov//4\n",
    "    column=mov%4\n",
    "    action=(row,column)\n",
    "    b.coord_move(action)\n",
    "    print(\"\\nQ-function of trained agent:\")\n",
    "    print(pred[0],\"\\t\",pred[1],\"\\t\" ,pred[2],\"\\t\",pred[3])\n",
    "    print(pred[4],\"\\t\", pred[5],\"\\t\", pred[6],\"\\t\", pred[7])\n",
    "    print(pred[8],\"\\t\", pred[9],\"\\t\", pred[10],\"\\t\", pred[11])\n",
    "    print(pred[12],\"\\t\", pred[13],\"\\t\", pred[14],\"\\t\", pred[15])\n",
    "    print(\"\\nQ-function of short trained agent:\")\n",
    "    print(predBad[0],\"\\t\",predBad[1],\"\\t\" ,predBad[2],\"\\t\",predBad[3])\n",
    "    print(predBad[4],\"\\t\", predBad[5],\"\\t\", predBad[6],\"\\t\", predBad[7])\n",
    "    print(predBad[8],\"\\t\", predBad[9],\"\\t\", predBad[10],\"\\t\", predBad[11])\n",
    "    print(predBad[12],\"\\t\", predBad[13],\"\\t\", predBad[14],\"\\t\", predBad[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rungames()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
