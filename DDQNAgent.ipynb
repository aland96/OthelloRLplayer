{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tree for implementation of Prioritized Experience Replay\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros( 2*capacity - 1 )\n",
    "        self.data = np.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class memory, using the Tree for PER\n",
    "class Memory:   # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "\n",
    "    def _getPriority(self, error):\n",
    "        #converting error in priority with this formula\n",
    "        #epsilon self.e is a small positive constant that ensures that no transition has zero priority\n",
    "        #alpha, 0≤ self.a ≤1, controls the difference between high and low error\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.add(p, sample) \n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        segment = self.tree.total() / n\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            batch.append( (idx, data) )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._getPriority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.memory_cap=200000\n",
    "        self.batch_size=32\n",
    "        self.gamma = 0.8\n",
    "        self.max_eps=1\n",
    "        #Higer min_eps for the start, tha smaller\n",
    "        self.min_eps=0.001\n",
    "        self.learning_rate = 0.00025\n",
    "        self.epsilon=1\n",
    "        self.steps=0\n",
    "        #smaller lambda for the start , than bigger\n",
    "        #Step 1\n",
    "        self.lambd=0.0005\n",
    "        self.update_target_freq= 5000\n",
    "        self.memory = Memory(self.memory_cap)\n",
    "        #self.model = self.network()\n",
    "        self.model = self.network(\"8x8_weights_new.hdf5\")\n",
    "        \n",
    "        #model for target network\n",
    "        #self.model_=self.network()\n",
    "        self.model_=self.network(\"8x8_weights_new.hdf5\")\n",
    "        \n",
    "        \n",
    "    def network(self, weights=None):\n",
    "        model = Sequential()        \n",
    "        model.add(Dense(640 , activation=\"relu\", input_shape=(64,)))\n",
    "        model.add(Dropout(0.20))\n",
    "        model.add(Dense(128 , activation=\"relu\", input_shape=(64,)))\n",
    "        model.add(Dropout(0.20))\n",
    "        model.add(Dense(64,activation='softmax'))\n",
    "        #I use the huber loss function\n",
    "        opt = RMSprop(lr=self.learning_rate)\n",
    "        model.compile(loss=tf.keras.losses.Huber(), optimizer=opt)\n",
    "\n",
    "        if weights:\n",
    "            model.load_weights(weights)\n",
    "        return model\n",
    "        \n",
    "    def get_state(self,board):\n",
    "        state_old=board.get_board_state_array()\n",
    "        state_old=np.where(state_old==1,2,state_old)\n",
    "        state_old=np.where(state_old==0,1,state_old)\n",
    "        state_old=np.where(state_old==None,0,state_old)\n",
    "        return state_old\n",
    "    \n",
    "    def makemove(self,board):\n",
    "        print(\"Predicted move:\")\n",
    "        state_old = self.get_state(board)\n",
    "        pred=self.predict(state_old.reshape((1,64)))[0]\n",
    "        mov=np.argmax(pred)\n",
    "        row=mov//8            \n",
    "        column=mov%8\n",
    "        action=(row,column)\n",
    "        return action\n",
    "    \n",
    "    #I give a +1 reward if i win and -1 if i lose, everything else is 0\n",
    "    def set_reward(self, board, action):\n",
    "        reward = 0\n",
    "        \n",
    "        d=board.get_game_over()\n",
    "        #the player is the opponent because i did the last action\n",
    "        pl=board.current_player\n",
    "        bl=board.get_black_score()\n",
    "        wh=board.get_white_score()\n",
    "        \n",
    "        if d==False:\n",
    "            cell = board.get_board_state()[action]\n",
    "            if cell.is_valid_move is True:\n",
    "                reward= 0.5\n",
    "            else:\n",
    "                reward= -1\n",
    "        else:\n",
    "            #two option if i win and i'm white or black\n",
    "            if(pl==1 and bl>wh):\n",
    "                reward= reward + 1\n",
    "            elif(pl==0 and wh>bl):\n",
    "                reward= reward + 1\n",
    "            else:\n",
    "                #here I've lost\n",
    "                reward= reward -1\n",
    "        return reward    \n",
    "    \n",
    "    #MEMORY\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        sample=(state, action, reward, next_state, done)\n",
    "        x,y,errors = self.replay_new([(0,sample)])\n",
    "        self.memory.add(errors[0],sample)\n",
    "        \n",
    "        #update target model\n",
    "        if self.steps % self.update_target_freq == 0:\n",
    "            self.updateTargetModel()\n",
    "          \n",
    "        self.steps+=1\n",
    "        self.epsilon = self.min_eps + (self.max_eps - self.min_eps) * math.exp(-self.lambd * self.steps)\n",
    "        \n",
    "    #PREDICTION\n",
    "    \n",
    "    def predict(self,s,target=False):\n",
    "        if target:\n",
    "            return self.model_.predict(s)\n",
    "        else:\n",
    "            return self.model.predict(s)\n",
    "    \n",
    "    def updateTargetModel(self):\n",
    "        self.model_.set_weights(self.model.get_weights())\n",
    "    \n",
    "    #TRAIN\n",
    "    \n",
    "    def replay_new(self,minibatch):        \n",
    "        #contain information for the all minibatch\n",
    "        x = []\n",
    "        y = []\n",
    "        errors = []\n",
    "        for index, sample in minibatch:\n",
    "            (state, action, reward, next_state, done) = sample\n",
    "            #getting prediction on states\n",
    "            pred=self.predict(state.reshape((1,64)))[0]\n",
    "            next_pred=self.predict(next_state.reshape((1,64)),target=False)[0]\n",
    "            next_pred_target=self.predict(next_state.reshape((1,64)),target=True)[0]\n",
    "\n",
    "            oldVal=pred[action[0]*8+action[1]]\n",
    "            \n",
    "            if not done:\n",
    "                #Bellman equation with double DQN\n",
    "                pred[action[0]*8+action[1]] = reward + self.gamma * next_pred_target[np.argmax(next_pred)]\n",
    "            else:\n",
    "                pred[action[0]*8+action[1]] = reward\n",
    "            \n",
    "            error= abs(oldVal - pred[action[0]*8+action[1]])\n",
    "            x.append(state)\n",
    "            y.append(pred)\n",
    "            errors.append(error)\n",
    "        return(x,y,errors)\n",
    "    \n",
    "    def replay(self):\n",
    "        minibatch = self.memory.sample(self.batch_size)\n",
    "        x,y,errors = self.replay_new(minibatch)\n",
    "        X = np.array(x)\n",
    "        Y = np.array(y)\n",
    "        #update errors\n",
    "        for i in range(len(minibatch)):\n",
    "            idx = minibatch[i][0]\n",
    "            self.memory.update(idx, errors[i])\n",
    "        \n",
    "        self.model.fit(X,Y,batch_size=self.batch_size, epochs=1, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
